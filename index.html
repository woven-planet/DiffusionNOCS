<!DOCTYPE html>
<html lang="en">

<script src="bootstrap.js"></script>
<script type="text/javascript" charset="utf-8" src="https://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script> 
<!---
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
--->
<script src="load-mathjax.js" async></script>


<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
}

@media screen and (min-width: 980px){
    body {
        width: 980px;
    }
}


h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #1468b7;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1 {
    text-align: center;
}
h2,h3 {
    text-align: left;
}

h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
h3 {
    font-weight: 600;
    margin: 16px 0px 4px 0px;
}

.paper-title {
    padding: 1px 0px 1px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-5 {
     width: 20%;
     float: left;
}

.move-down {
    margin-top:0.6cm;
}

.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.col-1 {
     width: 100%;
     float: left;
}

.col-8{
width: 12.5%;
}

.author-row, .affil-row {
    font-size: 26px;
}

.author-row-new { 
    text-align: center; 
}

.author-row-new a {
    display: inline-block;
    font-size: 20px;
    padding: 4px;
}

.author-row-new sup {
    color: #313436;
    font-size: 12px;
}

.affiliations-new {
    font-size: 18px;
    text-align: center;
    width: 80%;
    margin: 0 auto;
    margin-bottom: 20px;
}

.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 26px;
}
.affiliatons {
    font-size: 18px;
}
.affil-row {
    margin-top: 18px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    color: #666;
    margin-top: 4px;
    margin-bottom: 10px;
	text-align: left;
}


video {
    display: block;
    margin: auto;
}


figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    padding: 0;
    margin: 0;
    list-style: none;
}

.paper-btn-coming-soon {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 15px;
  
  background-color: #838383;
  color: white !important;
  font-size: 20px;
  width: 100px;
  font-weight: 200;
}

.coming-soon {
    position: absolute;
    top: -15px;
    right: -15px;
}

.center {
  margin-left: 10.0%;
  margin-right: 10.0%;
}

.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 15px;
  
  background-color: #3e3131;
  color: white !important;
  font-size: 20px;
  width: 100px;
  font-weight: 200;
}
.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}

.paper-btn:hover {
    opacity: 0.85;
}

.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}

.venue {
    font-size: 23px;
}

.topnav {
    background-color: #EEEEEE;
    overflow: hidden;
}

.topnav div {
    max-width: 1070px;
    margin: 0 auto;
}

.topnav a {
    display: inline-block;
    color: black;
    text-align: center;
    vertical-align: middle;
    padding: 16px 16px;
    text-decoration: none;
    font-size: 18px;
}

.topnav img {
    padding: 2px 0px;
    width: 100%;
    margin: 0.2em 0px 0.3em 0px;
    vertical-align: middle;
}

pre {
    font-size: 0.9em;
    padding-left: 7px;
    padding-right: 7px;
    padding-top: 3px;
    padding-bottom: 3px;
    border-radius: 3px;
    background-color: rgb(235, 235, 235);
    overflow-x: auto;
}

.download-thumb {
    display: flex;
}

@media only screen and (max-width: 620px) {
    .download-thumb {
        display: none;
    }
}

.paper-stuff {
    width: 50%;
    font-size: 20px;
}

@media only screen and (max-width: 620px) {
    .paper-stuff {
        width: 100%;
    }
}
* {
  box-sizing: border-box;
}

.column {
  text-align: center;
  float: left;
  width: 16.666%;
  padding: 5px;
}
.column3 {
  text-align: center;
  float: left;
  width: 33.333%;
  padding: 5px;
}
.column4 {
  text-align: center;
  float: left;
  width: 50%;
  padding: 5px;
}
.column5 {
  text-align: center;
  float: left;
  width: 20%;
  padding: 5px;
}
.column10 {
  text-align: center;
  float: left;
  width: 10%;
  padding: 5px;
}
.border-right {
    border-right: 1px solid black;
}
.border-bottom{
    border-bottom: 1px solid black;
}


.row-center {
    margin: 16px 0px 16px 0px;
    text-align: center;
}

/* Clearfix (clear floats) */
.row::after {
  content: "";
  clear: both;
  display: table;
}
.img-fluid {
  max-width: 100%;
  height: auto;
}
.figure-img {
  margin-bottom: 0.5rem;
  line-height: 1;
}








.rounded-circle {
  border-radius: 50% !important;
}






/* Responsive layout - makes the three columns stack on top of each other instead of next to each other */
@media screen and (max-width: 500px) {
  .column {
    width: 100%;
  }
}
@media screen and (max-width: 500px) {
  .column3 {
    width: 100%;
  }
}

</style>
<link rel="stylesheet" href="bootstrap-grid.css">
<link rel="stylesheet" href="simplegrid.css">

<script type="text/javascript" src="../js/hidebib.js"></script>
    <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
    <head>
        <title> DiffusionNOCS: Managing Symmetry and Uncertainty in Sim2Real Multi-Modal Category-level Pose Estimation</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:description" content="Wide baseline novel view synthesis using learned priors"/>
        <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:creator" content="">
        <meta name="twitter:title" content="DiffusionNOCS: Managing Symmetry and Uncertainty in Sim2Real Multi-Modal Category-level Pose Estimation">
        <meta name="twitter:description" content="">
        <meta name="twitter:image" content="">
    </head>

<body>

<div class="container">
    <div class="paper-title">
    <h1> 
        DiffusionNOCS: Managing Symmetry and Uncertainty in Sim2Real Multi-Modal Category-level Pose Estimation
    </div>

    <div id="authors">
        <center>
            <div class="author-row-new">
                <a href="https://www.linkedin.com/in/takuya-ikeda-a66132190/">Takuya Ikeda<sup>1,*</sup></a>,
                <a href="https://zakharos.github.io/">Sergey Zakharov<sup>2,*</sup></a>,
                <a href="https://www.linkedin.com/in/%E5%A4%A9%E6%AF%85-%E5%BA%B7-8a3a68207/">Tianyi Ko<sup>1</sup></a>,
                <a href="https://zubairirshad.com">Muhammad Zubair Irshad<sup>2</sup></a>,
                <a href="https://www.linkedin.com/in/robert-lee-a8a98922b/">Robert Lee<sup>1</sup></a>,
                <a href="https://www.thekatherineliu.com/">Katherine Liu<sup>2</sup></a>,
                <a href="https://www.tri.global/about-us/dr-rares-ambrus/">Rares Ambrus<sup>2</sup></a>,
                <a href="https://www.linkedin.com/in/knishiwaki">Koichi Nishiwaki<sup>2</sup></a>
            </div>
        </center>
        <center>
        <div class="affiliations">
            <span class="author-block"><sup>1</sup> <a href="https://woven.toyota/" style="vertical-align:middle"><img style="width:30px" src="img/woven.png"></a> Woven by Toyota,</span>
                <span class="author-block"><sup>2</sup> <a href="https://woven.toyota/" style="vertical-align:middle"><img style="width:27px" src="img/tri.png"></a> Toyota Research Institute,  </span>
            <span><sup>*</sup> indicates equal contribution</span>
            <!-- <a href="https://woven.toyota/"><img style="width:25%; padding-right: 15px;" src="img/woven_logo.svg"> </a> -->
            <!-- <a href="https://www.tri.global/"><img style="width:25%; padding-right: 15px;" src="img/tri_logo.svg"> </a> -->
        </div>

        <!-- <div class="affil-row">
            <div class="venue text-center"><b>CVPR 2023 </b></div>
        </div> -->

        </center>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="paper-btn" href="https://arxiv.org/abs/2402.12647">
                <span class="material-icons"> feed </span> 
                 Paper
            </a>
            <a class="paper-btn-coming-soon" href="https://github.com/woven-planet">
                <span class="material-icons"> code </span>
                Code
            </a>
            <a class="paper-btn-coming-soon" href="#data">
                <span class="material-icons"> storage </span>
                Data
            </a>
            </div>
        </div>
    </div>

    <section id="teaser-image">
        <center>
            <figure>
                <video class="centered" width="85%" autoplay loop muted playsinline class="video-background " >
                    <source src="img/demo.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <p class="caption, centered">
                    Poses generated by DiffusionNOCS used for placement in a real robotics setting.
                 </p>
            </figure>

        </center>
    </section>

    <section id="abstract"/>
        <h2>Abstract</h2>
        <hr>
        <div class="flex-row">
            <p>
                This work addresses the challenging problem of
                category-level pose estimation. Current state-of-the-art methods
                for this task face challenges when dealing with symmetric
                objects and when attempting to generalize to new environments
                solely through synthetic data training. In this work, we address
                these challenges by proposing a probabilistic model that
                relies on diffusion to estimate dense canonical maps crucial
                for recovering partial object shapes as well as establishing
                correspondences essential for pose estimation. Furthermore,
                we introduce critical components to enhance performance by
                leveraging the strength of the diffusion models with multi-modal
                 input representations. We demonstrate the effectiveness
                of our method by testing it on a range of real datasets.
                Despite being trained solely on our generated synthetic data,
                our approach achieves state-of-the-art performance and unprecedented generalization qualities, outperforming baselines,
                even those specifically trained on the target domain.
            </p>
        </div>
    </section>


    <div class="section">
        <h2>Method</h2>
        <hr>
        <!-- <p>
            Below, we illustrate novel view rendering results of our approach from different wide-baseline image
            pairs. Our approach is able to consistently render different wide baseline novel views.
        </p> -->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/method.mp4" type="video/mp4">
                </video>
            </div> 
        </div>
    </div>
    <br>

    <div class="section">
        <h2>Handling Symmetry</h2>
        <hr>
        <p>
            Thanks to its <b>probabilistic nature</b>, <span class="method">DiffusionNOCS</span> can handle <b>symmetrical objects</b> 
            without a need for special data annotations and heuristics typical for many SOTA methods. 
        </p>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/symmetry.mp4" type="video/mp4">
                </video>
            </div> 
        </div>
    </div>
    <br>

    <div class="section">
        <h2>Selectable Inputs</h2>
        <hr>
        <p>
            A single network can be used to generate reconstructions from various inputs without re-training 
            since our method supports <b>selectable inputs</b>.
        </p>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/inputs.mp4" type="video/mp4">
                </video>
            </div> 
        </div>
    </div>
    <br>

    <div class="section">
        <h2>NOCS Real 275 Benchmark</h2>
        <hr>
        <p>
            <span class="dnerf">DiffusionNOCS</span> shows the best results across SOTA baselines trained on <b>synthetic</b> data on a de facto standard
            benchmark for category-level pose estimation, NOCS Real (<a href="https://github.com/hughw19/NOCS_CVPR2019">Wang et al., 2019</a>).
        </p>
    </div>
    <div class="row align-items-center">
        <div class="col justify-content-center text-center">
            <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                <source src="img/nocs.mp4" type="video/mp4">
            </video>
        </div> 
    </div>
    <br>

    <div class="section" id="data">
        <h2>Generalization Benchmark</h2>
        <hr>
        <p>
            To demonstrate how existing state-of-the-art (SOTA) methods perform in 
            various challenging real-world environments, we introduce a zero-shot 
            <b>Generalization Benchmark</b> consisting of three datasets commonly used for instance-level
            pose estimation, YCB-V (<a href="https://rse-lab.cs.washington.edu/projects/posecnn/">Xiang et al., 2018</a>), 
            HOPE (<a href="https://github.com/swtyree/hope-dataset">Tyree et al., 2022</a>), 
            and TYO-L (<a href="https://bop.felk.cvut.cz/datasets/">Hodan et al., 2018</a>).
            We show the best overall performance even when compared to methods trained on <b>real</b> data.
        </p>
        <!-- <p>
            Download the datasets from <a href="https://bop.felk.cvut.cz/datasets/">BOP benchmark webpage</a> and run our <a href="https://github.com/woven-planet">postprocessing script</a> to standardize object models to be
            consistently canonicalized with the NOCS dataset.
        </p> -->
    </div>
    <div class="row align-items-center">
        <div class="col justify-content-center text-center">
            <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                <source src="img/generalization.mp4" type="video/mp4">
            </video>
        </div> 
    </div>
    <br>

    <!-- <div class="section">
        <h2>Paper</h2>
        <hr>
        <p>
        </p>
        <div>
            <div class="list-group">
                <a href=""
                   class="list-group-item">
                    <img src="img/paper_thumbnail.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
                </a>
            </div>
        </div>
    </div> -->

        <div class="section">
            <h2>Our Related Projects</h2>
            <hr>
            <!-- <p>
                Check out our related projects on neural rendering and neural fields! <br>
            </p> -->


            <div class='row vspace-top'>
                <div class="col-sm-3">
                <img src='img/related/bingham.png' class='img-fluid'>
                </div>
    
                <div class="col">
                <div class='paper-title'>
                    <a href="https://arxiv.org/abs/2203.04456">A Probabilistic Rotation Representation for Symmetric Shapes With an Efficiently Computable Bingham Loss Function</a>
                </div>
                <div>
                    While quaternion is a common choice for rotation representation, it cannot represent the ambiguity of the observation. In order to handle the ambiguity, the Bingham distribution is one promising solution. However, it requires complicated calculation when yielding the negative log-likelihood (NLL) loss. In this paper, we introduce a fast-computable and easy-to-implement NLL loss function for Bingham distribution. We also create the inference network and show that our loss function can capture the symmetric property of target objects from their point clouds.
                </div>
                </div>
            </div>

            <div class='row vspace-top'>
                <div class="col-sm-3">
                    <img src='img/related/sim2real.png' class='img-fluid'>
                </div>

                <div class="col">
                    <div class='paper-title'>
                        <a href="https://arxiv.org/abs/2203.02069"> Sim2Real Instance-Level Style Transfer for 6D Pose Estimation</a>

                    </div>
                    <div>
                        In recent years, synthetic data has been widely used in the training of 6D pose estimation networks, in part because it automatically provides perfect annotation at low cost. However, there are still non-trivial domain gaps, such as differences in textures/materials, between synthetic and real data. To solve this problem, we introduce a simulation to reality (sim2real) instance-level style transfer for 6D pose estimation network training. Our approach transfers the style of target objects individually, from synthetic to real, without human intervention. 
                    </div>
                </div>
            </div>     
            
            <div class='row vspace-top'>
                <div class="col-sm-3">
                    <img src='img/related/shapo.png' class='img-fluid'>
                </div>

                <div class="col">
                    <div class='paper-title'>
                        <a href="https://zubair-irshad.github.io/projects/ShAPO.html"> ShAPO: Implicit Representations for Multi-Object Shape Appearance and Pose Optimization</a>

                    </div>
                    <div>
                        In this work, we study the complex task of holistic object-centric 3D understanding from a single RGB-D observation. As it is an ill-posed problem, existing methods suffer from low performance for both 3D shape and 6D pose estimation in complex multi-object scenarios with occlusions. We present ShAPO, a method for joint multi-object detection, 3D textured reconstruction, 6D object pose and size estimation. Our
                        method detects and reconstructs novel objects without having access to their
                        ground truth 3D meshes.
                    </div>
                </div>
            </div> 
            
            <!-- <div class='row vspace-top'>
                <div class="col-sm-3">
                    <div class="move-down">
                        <img src='img/related/convmae.png' class='img-fluid'>
                    </div>
                </div>

                <div class="col">
                    <div class='paper-title'>
                        <a href="https://arxiv.org/abs/2403.14628">Zero-Shot Multi-Object Shape Completion</a>

                    </div>
                    <div>
                        We present a 3D shape completion method that recovers the complete geometry of multiple objects in complex scenes from a single RGB-D image. To generalize to a wide range of objects in diverse scenes, we create a large-scale photorealistic dataset, featuring a diverse set of 12K 3D object models from the Objaverse dataset which are rendered in multi-object scenes with physics-based positioning. Our method outperforms the current state-of-the-art on both synthetic and real-world datasets and demonstrates a strong zero-shot capability.
                    </div>
                </div>
            </div> -->

            <!-- <div class='row vspace-top'>
                <div class="col-sm-3">
                    <img src='img/related/powergrasp.png' class='img-fluid'>
                </div>

                <div class="col">
                    <div class='paper-title'>
                        <a href="https://arxiv.org/abs/2312.11804">Gravity-aware Grasp Generation with Implicit Grasp Mode Selection for Underactuated Hands</a>

                    </div>
                    <div>
                        Learning-based grasp detectors typically assume a precision grasp, where each finger only has one contact point, and estimate the grasp probability. In this work, we propose a data generation and learning pipeline that can leverage power grasping, which has more contact points with an enveloping configuration and is robust against both positioning error and force disturbance. 
                    </div>
                </div>
            </div> 





    <!-- <hr> -->

    <section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre><code>@inproceedings{diffusionnocs,
                    title={DiffusionNOCS: Managing Symmetry and Uncertainty in Sim2Real Multi-Modal Category-level Pose Estimation},
                    author={Takuya Ikeda, Sergey Zakharov, Tianyi Ko, Muhammad Zubair Irshad, Robert Lee, Katherine Liu, Rares Ambrus, Koichi Nishiwaki},
                    journal={arXiv},
                    year={2024}
                }</code></pre>
    </section>

    <!-- <section> 
        This webpage template was adapted from <a href='https://nv-tlabs.github.io/LION/'>here</a>.
    </section> -->

</div>

</body>
</html>
